{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea1c6e84-1281-4bb6-a297-48cb03ffc575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data exported to 'cowpresso.csv'.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "def clean_timestamp(timestamp):\n",
    "    \"\"\"Convert timestamp to a cleaner format: 'YYYY-MM-DD'.\"\"\"\n",
    "    try:\n",
    "        dt = datetime.strptime(timestamp, \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "        return dt.strftime(\"%Y-%m-%d\")\n",
    "    except (ValueError, TypeError):\n",
    "        return timestamp  # Return original if it can't be parsed\n",
    "\n",
    "def extract_coffee_details(body_text):\n",
    "    \"\"\"\n",
    "    Extract coffee details dynamically from HTML body text.\n",
    "    Returns a dictionary with potential keys (or None if not found).\n",
    "    \"\"\"\n",
    "    fields = {\n",
    "        \"Region\": None,\n",
    "        \"Variety\": None,\n",
    "        \"Elevation\": None,\n",
    "        \"Processing\": None,\n",
    "        \"Roast profile\": None,\n",
    "        \"Flavour notes\": None,\n",
    "        \"Acidity\": None,\n",
    "        \"Body\": None,\n",
    "        \"Tasting Experience\": None,\n",
    "        \"Farm Information\": None,\n",
    "        \"Moisture content of Green Coffee\": None,\n",
    "        \"Packaging\": None\n",
    "    }\n",
    "    \n",
    "    patterns = {\n",
    "        \"Region\": r\"Region:\\s*([\\w\\s.,&-]+)\",\n",
    "        \"Variety\": r\"Variety:\\s*([\\w\\s.,&-]+)\",\n",
    "        \"Elevation\": r\"Elevation:\\s*([\\w\\s.,&-]+)\",\n",
    "        \"Processing\": r\"Processing:\\s*([\\w\\s.,&-]+)\",\n",
    "        \"Roast profile\": r\"Roast profile:\\s*([\\w\\s.,&-]+)\",\n",
    "        \"Flavour notes\": r\"Flavour notes:\\s*([\\w\\s.,&-]+)\",\n",
    "        \"Acidity\": r\"Acidity:\\s*([\\w\\s.,&-]+)\",\n",
    "        \"Body\": r\"Body:\\s*([\\w\\s.,&-]+)\",\n",
    "        \"Tasting Experience\": r\"Tasting experience:\\s*([\\w\\s.,&-]+)\",\n",
    "        \"Farm Information\": r\"Farm Information:\\s*([\\w\\s.,&-]+)\",\n",
    "        \"Moisture content of Green Coffee\": r\"Moisture content of Green Coffee:\\s*([\\w\\s.,&-]+)\",\n",
    "        \"Packaging\": r\"Packaging:\\s*([\\w\\s.,&-]+)\"\n",
    "    }\n",
    "    \n",
    "    soup = BeautifulSoup(body_text, \"html.parser\")\n",
    "    plain_text = soup.get_text()\n",
    "\n",
    "    for field, pattern in patterns.items():\n",
    "        match = re.search(pattern, plain_text, re.IGNORECASE | re.DOTALL)\n",
    "        if match:\n",
    "            fields[field] = match.group(1).strip()\n",
    "    return fields\n",
    "\n",
    "def fetch_products(page):\n",
    "    \"\"\"Fetch products from the paginated API.\"\"\"\n",
    "    url = f\"https://cowpressocoffee.sg/collections/frontpage/products.json?page={page}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data.get(\"products\", [])\n",
    "    return []\n",
    "\n",
    "def parse_product(product):\n",
    "    \"\"\"\n",
    "    Parse a single product and return a list of rows (one per variant)\n",
    "    with the common product details and variant-specific information.\n",
    "    Only include variants with 'Whole Beans' in the title.\n",
    "    \"\"\"\n",
    "    description_html = product.get(\"body_html\", \"\")\n",
    "    extracted_details = extract_coffee_details(description_html)\n",
    "    \n",
    "    rows = []\n",
    "    # Use first image URL (if available) for all variants\n",
    "    images = product.get(\"images\", [])\n",
    "    image_url = images[0].get(\"src\", \"\") if images else \"\"\n",
    "    \n",
    "    for variant in product.get(\"variants\", []):\n",
    "        variant_title = variant.get(\"title\", \"\").lower()\n",
    "        if \"whole beans\" in variant_title:  # Filter for Whole Beans\n",
    "            row = {\n",
    "                \"Product ID\": product.get(\"id\"),\n",
    "                \"Title\": product.get(\"title\"),\n",
    "                \"Vendor\": product.get(\"vendor\"),\n",
    "                \"Created At\": clean_timestamp(product.get(\"created_at\", \"\")),\n",
    "                \"Updated At\": clean_timestamp(product.get(\"updated_at\", \"\")),\n",
    "                \"Variant ID\": variant.get(\"id\"),\n",
    "                \"Variant Title\": variant.get(\"title\"),\n",
    "                \"Variant Price\": variant.get(\"price\"),\n",
    "                \"Image URL\": image_url,\n",
    "            }\n",
    "            # Merge extracted details\n",
    "            row.update(extracted_details)\n",
    "            rows.append(row)\n",
    "    return rows\n",
    "\n",
    "def clean_row(row):\n",
    "    \"\"\"\n",
    "    Clean a row by applying regex fixes for misplaced headers\n",
    "    and remove unwanted fields.\n",
    "    \"\"\"\n",
    "    misplaced_headers = [\n",
    "        \"Elevation\", \"Variety\", \"Processing\", \"Roast profile\",\n",
    "        \"Flavour notes\", \"Acidity\", \"Body\", \"Tasting notes\", \n",
    "        \"District\", \"Farm\", \"Packaging\", \"Type of Soil\", \"Soil Type\", \n",
    "        \"MASL\", \"Varietal\", \"Flavour\", \"Average Annual Rainfall\", \n",
    "        \"notes\", \"this crop\", \"Roast Level\", \"Process\"\n",
    "    ]\n",
    "    # Process each cell\n",
    "    for key, value in row.items():\n",
    "        if value and isinstance(value, str):\n",
    "            for header in misplaced_headers:\n",
    "                # Insert a space before header if needed, then remove the header text\n",
    "                value = re.sub(rf\"(?<!\\s){header}\", rf\" {header}\", value, flags=re.IGNORECASE)\n",
    "                value = re.sub(rf\"\\b{header}\\b\", \"\", value, flags=re.IGNORECASE)\n",
    "            if key == \"Elevation\":\n",
    "                value = re.sub(r\"[^\\d\\s,-]\", \"\", value).strip()\n",
    "            value = re.sub(r\"\\s{2,}\", \" \", value).strip()\n",
    "            row[key] = value\n",
    "    # Drop unwanted columns\n",
    "    columns_to_drop = [\"Tasting Experience\", \"Farm Information\", \n",
    "                       \"Moisture content of Green Coffee\", \"Packaging\", \"Body\"]\n",
    "    for col in columns_to_drop:\n",
    "        row.pop(col, None)\n",
    "    return row\n",
    "\n",
    "def scrape_and_clean():\n",
    "    \"\"\"Scrape the products, clean the data, and write final CSV 'cowpresso.csv'.\"\"\"\n",
    "    page = 1\n",
    "    all_products = []\n",
    "    while True:\n",
    "        products = fetch_products(page)\n",
    "        if not products:\n",
    "            break\n",
    "        for product in products:\n",
    "            rows = parse_product(product)\n",
    "            all_products.extend(rows)\n",
    "        page += 1\n",
    "\n",
    "    # Define final output header (order matters)\n",
    "    final_headers = [\n",
    "        \"Product ID\", \"Title\", \"Vendor\", \"Created At\", \"Updated At\",\n",
    "        \"Variant ID\", \"Variant Title\", \"Variant Price\", \"Image URL\",\n",
    "        \"Region\", \"Variety\", \"Elevation\", \"Processing\", \n",
    "        \"Roast profile\", \"Flavour notes\", \"Acidity\"\n",
    "    ]\n",
    "    \n",
    "    # Clean each row and ensure only final_headers remain\n",
    "    cleaned_products = []\n",
    "    for row in all_products:\n",
    "        cleaned = clean_row(row)\n",
    "        # Build a new dict with only the final headers (use empty string if missing)\n",
    "        cleaned_products.append({col: cleaned.get(col, \"\") for col in final_headers})\n",
    "    \n",
    "    # Write the final CSV\n",
    "    output_file = \"cowpresso.csv\"\n",
    "    with open(output_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=final_headers, quoting=csv.QUOTE_MINIMAL, escapechar=\"\\\\\")\n",
    "        writer.writeheader()\n",
    "        for row in cleaned_products:\n",
    "            writer.writerow(row)\n",
    "    \n",
    "    print(f\"Cleaned data exported to '{output_file}'.\")\n",
    "\n",
    "# Run the complete scraping and cleaning process\n",
    "scrape_and_clean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

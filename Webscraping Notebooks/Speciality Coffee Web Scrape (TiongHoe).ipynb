{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "908bb730-33eb-434f-83d5-99c2eee3b45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variant data successfully written to tionghoe_variant_data.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import json\n",
    "\n",
    "# Base URLs\n",
    "collection_url = \"https://tionghoe.com/collections/roasted-beans\"\n",
    "\n",
    "# Headers to mimic a browser visit\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Output CSV file names\n",
    "card_data_file = \"tionghoe_card_data.csv\"\n",
    "variant_data_file = \"tionghoe_variant_data.csv\"\n",
    "\n",
    "# Request the collection page content\n",
    "response = requests.get(collection_url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Step 2: Extract and process the meta JSON for variants\n",
    "variant_data = []\n",
    "meta_data_script = soup.find(\"script\", string=lambda t: t and \"var meta =\" in t)\n",
    "if meta_data_script:\n",
    "    # Extract the JSON text\n",
    "    meta_data_text = meta_data_script.string\n",
    "\n",
    "    # Isolate the JSON object by finding the first valid curly brace\n",
    "    start_index = meta_data_text.find('{\"products\":')  # Find the start of the JSON\n",
    "    end_index = meta_data_text.rfind(\"};\") + 1          # Find the last closing brace\n",
    "    valid_json_text = meta_data_text[start_index:end_index]\n",
    "\n",
    "    try:\n",
    "        meta = json.loads(valid_json_text)  # Parse the isolated JSON\n",
    "        for product in meta.get(\"products\", []):\n",
    "            product_title = product.get(\"vendor\", \"N/A\")\n",
    "            product_type = product.get(\"type\", \"N/A\")\n",
    "            product_id = product.get(\"id\", \"N/A\")\n",
    "\n",
    "            for variant in product.get(\"variants\", []):\n",
    "                variant_price = variant.get(\"price\", 0) / 100\n",
    "                variant_name = variant.get(\"name\", \"N/A\")\n",
    "\n",
    "                # Only keep variants with \"Whole Beans\" in the title\n",
    "                if \"Whole Beans\" in variant_name:\n",
    "                    variant_url = f\"https://tionghoe.com/products/{product_id}\"\n",
    "\n",
    "                    # Add variant data\n",
    "                    variant_data.append({\n",
    "                        \"Title\": f\"{product_title} - {variant_name}\",\n",
    "                        \"Product URL\": variant_url,\n",
    "                        \"Product Type\": product_type,\n",
    "                        \"Price (SGD)\": f\"S${variant_price:.2f}\"\n",
    "                    })\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "\n",
    "# Write variant data to CSV\n",
    "if variant_data:\n",
    "    with open(variant_data_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        fieldnames = [\"Title\", \"Product URL\", \"Product Type\", \"Price (SGD)\"]\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "\n",
    "        # Write header and rows\n",
    "        writer.writeheader()\n",
    "        writer.writerows(variant_data)\n",
    "\n",
    "    print(f\"Variant data successfully written to {variant_data_file}\")\n",
    "else:\n",
    "    print(\"No variant data to write to CSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b7dc7c49-c61d-497b-8e96-a37805687321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blend: 50% India and 50% Brazil\n",
      "Acidity Level: Low\n",
      "Roast Level: Medium\n",
      "Flavor: Hazelnuts, Caramel, Brown Sugar\n",
      "Body: Heavy-bodied, Creamy\n",
      "Aftertaste: Dark Chocolates, Roasted Peanuts\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the product page\n",
    "url = \"https://tionghoe.com/products/smoky-quartz-seasonal-espresso-blend\"\n",
    "\n",
    "# Headers to mimic a browser visit\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Fetch the page content\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Extract blend, acidity level, and roast level from the description\n",
    "description_div = soup.find(\"div\", class_=\"section-stack__intro\")\n",
    "\n",
    "if description_div:\n",
    "    description_text = description_div.find(\"p\").get_text(separator=\" \")\n",
    "\n",
    "    # Safely extract values with error handling\n",
    "    try:\n",
    "        blend = description_text.split(\"Blend of:\")[1].split(\"Acidity Level:\")[0].strip()\n",
    "    except IndexError:\n",
    "        blend = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        acidity_level = description_text.split(\"Acidity Level:\")[1].split(\"Roast Level:\")[0].strip()\n",
    "    except IndexError:\n",
    "        acidity_level = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        roast_level = description_text.split(\"Roast Level:\")[1].split(\"Following our gemstone series\")[0].strip()\n",
    "    except IndexError:\n",
    "        roast_level = \"N/A\"\n",
    "else:\n",
    "    print(\"Description not found!\")\n",
    "    blend = \"N/A\"\n",
    "    acidity_level = \"N/A\"\n",
    "    roast_level = \"N/A\"\n",
    "\n",
    "# Extract flavor, body, and aftertaste from the feature chart\n",
    "feature_chart = soup.find(\"div\", class_=\"feature-chart__table\")\n",
    "if feature_chart:\n",
    "    try:\n",
    "        flavor = feature_chart.find(\"div\", string=\"Flavor\").find_next_sibling(\"div\").text.strip()\n",
    "    except AttributeError:\n",
    "        flavor = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        body = feature_chart.find(\"div\", string=\"Body\").find_next_sibling(\"div\").text.strip()\n",
    "    except AttributeError:\n",
    "        body = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        aftertaste = feature_chart.find(\"div\", string=\"Aftertaste\").find_next_sibling(\"div\").text.strip()\n",
    "    except AttributeError:\n",
    "        aftertaste = \"N/A\"\n",
    "else:\n",
    "    print(\"Feature chart not found!\")\n",
    "    flavor = \"N/A\"\n",
    "    body = \"N/A\"\n",
    "    aftertaste = \"N/A\"\n",
    "\n",
    "# Print results\n",
    "print(f\"Blend: {blend}\")\n",
    "print(f\"Acidity Level: {acidity_level}\")\n",
    "print(f\"Roast Level: {roast_level}\")\n",
    "print(f\"Flavor: {flavor}\")\n",
    "print(f\"Body: {body}\")\n",
    "print(f\"Aftertaste: {aftertaste}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "09c264b5-8245-440e-811e-70331394bd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: https://tionghoe.com/products/smoky-quartz-seasonal-espresso-blend\n",
      "Scraped: Smoky Quartz (Seasonal Espresso Blend)\n",
      "Scraping: https://tionghoe.com/products/jet-candy-seasonal-espresso-blend\n",
      "Scraped: Jet Candy (Seasonal Espresso Blend)\n",
      "Scraping: https://tionghoe.com/products/gachala-house-espresso-blend\n",
      "Scraped: Gachala (House Espresso Blend)\n",
      "Scraping: https://tionghoe.com/products/brazil-mogiana\n",
      "Scraped: Brazil Mogiana\n",
      "Scraping: https://tionghoe.com/products/ethiopia-alo-coffee-natural-g1\n",
      "Scraped: Ethiopia Alo Coffee Natural G1\n",
      "Scraping: https://tionghoe.com/products/indonesia-sumatra-mandheling\n",
      "Scraped: Indonesia Sumatra Mandheling\n",
      "Scraping: https://tionghoe.com/products/purple-onyx-seasonal-espresso-blend\n",
      "Scraped: Purple Onyx (Seasonal Espresso Blend)\n",
      "Scraping: https://tionghoe.com/products/colombia-popayan-decaf\n",
      "Scraped: Colombia Popayan Decaf\n",
      "Scraping: https://tionghoe.com/products/india-monsooned-malabar\n",
      "Scraped: India Monsooned Malabar\n",
      "Scraping: https://tionghoe.com/products/ethiopia-raro-nansebo\n",
      "Scraped: Ethiopia Raro Nansebo\n",
      "Scraping: https://tionghoe.com/products/ethiopia-alo-coffee-tamiru-experimental\n",
      "Scraped: Ethiopia Alo Coffee Tamiru Experimental\n",
      "Scraping: https://tionghoe.com/products/kenya-kianjuki-factory-aa\n",
      "Scraped: Kenya Kianjuki Factory AA\n",
      "Scraping: https://tionghoe.com/products/burundi-heza-mikuba\n",
      "Scraped: Burundi Heza Mikuba\n",
      "Scraping: https://tionghoe.com/products/ethiopia-bookkisa-shaakkisoo\n",
      "Scraped: Ethiopia Bookkisa Shaakkisoo\n",
      "Scraping: https://tionghoe.com/products/guatemala-san-andres-gesha\n",
      "Scraped: Guatemala San Andres Gesha\n",
      "Scraping: https://tionghoe.com/products/marigold-moonstone-seasonal-espresso-blend\n",
      "Scraped: Marigold Moonstone (Seasonal Espresso Blend)\n",
      "Scraping: https://tionghoe.com/products/colombia-heriberto-moreno\n",
      "Scraped: Colombia Heriberto Moreno\n",
      "Scraping: https://tionghoe.com/products/costa-rica-aquiares-estate\n",
      "Scraped: Costa Rica Aquiares Estate\n",
      "Product details successfully written to tionghoe_product_details.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# Base URLs\n",
    "collection_url = \"https://tionghoe.com/collections/roasted-beans\"\n",
    "base_product_url = \"https://tionghoe.com\"\n",
    "\n",
    "# Headers to mimic a browser visit\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Output CSV file\n",
    "output_file = \"tionghoe_product_details.csv\"\n",
    "\n",
    "# Step 1: Scrape product details from the collection page\n",
    "response = requests.get(collection_url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find all product cards\n",
    "product_cards = soup.select(\"span.product-card__title\")\n",
    "\n",
    "# Extract product data\n",
    "product_data = []\n",
    "for card in product_cards:\n",
    "    # Extract product title and URL\n",
    "    title_element = card.find(\"a\", class_=\"bold\")\n",
    "    title = title_element.text.strip() if title_element else \"N/A\"\n",
    "    product_url = title_element[\"href\"] if title_element else \"N/A\"\n",
    "    full_product_url = f\"{base_product_url}{product_url}\"\n",
    "\n",
    "    # Extract price\n",
    "    price_element = card.find_next(\"sale-price\")\n",
    "    price = price_element.text.strip() if price_element else \"N/A\"\n",
    "\n",
    "    # Append base data\n",
    "    product_data.append({\n",
    "        \"Title\": title,\n",
    "        \"Product URL\": full_product_url,\n",
    "        \"Price\": price\n",
    "    })\n",
    "\n",
    "# Step 2: Scrape additional details from each product page\n",
    "detailed_product_data = []\n",
    "for product in product_data:\n",
    "    print(f\"Scraping: {product['Product URL']}\")\n",
    "\n",
    "    # Fetch the product page\n",
    "    product_response = requests.get(product[\"Product URL\"], headers=headers)\n",
    "    product_soup = BeautifulSoup(product_response.text, \"html.parser\")\n",
    "\n",
    "    # Initialize all columns with default \"N/A\"\n",
    "    product_details = {\n",
    "        \"Title\": product[\"Title\"],\n",
    "        \"Product URL\": product[\"Product URL\"],\n",
    "        \"Price\": product[\"Price\"],\n",
    "        \"Blend\": \"N/A\",\n",
    "        \"Acidity Level\": \"N/A\",\n",
    "        \"Roast Level\": \"N/A\",\n",
    "        \"Flavor\": \"N/A\",\n",
    "        \"Body\": \"N/A\",\n",
    "        \"Aftertaste\": \"N/A\",\n",
    "        \"Region\": \"N/A\",\n",
    "        \"Varietal\": \"N/A\",\n",
    "        \"Process\": \"N/A\"\n",
    "    }\n",
    "\n",
    "    # Extract description details\n",
    "    description_div = product_soup.find(\"div\", class_=\"section-stack__intro\")\n",
    "    if description_div:\n",
    "        description_text = description_div.find(\"p\").get_text(separator=\" \")\n",
    "\n",
    "        try:\n",
    "            product_details[\"Blend\"] = description_text.split(\"Blend of:\")[1].split(\"Acidity Level:\")[0].strip()\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            product_details[\"Acidity Level\"] = description_text.split(\"Acidity Level:\")[1].split(\"Roast Level:\")[0].strip()\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            product_details[\"Roast Level\"] = description_text.split(\"Roast Level:\")[1].split(\"<br><br>\")[0].strip()\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            product_details[\"Region\"] = description_text.split(\"Region:\")[1].split(\"Varietal:\")[0].strip()\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            product_details[\"Varietal\"] = description_text.split(\"Varietal:\")[1].split(\"Process:\")[0].strip()\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            product_details[\"Process\"] = description_text.split(\"Process:\")[1].split(\"<br><br>\")[0].strip()\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "    # Extract flavor, body, and aftertaste from the feature chart\n",
    "    feature_chart = product_soup.find(\"div\", class_=\"feature-chart__table\")\n",
    "    if feature_chart:\n",
    "        try:\n",
    "            product_details[\"Flavor\"] = feature_chart.find(\"div\", string=\"Flavor\").find_next_sibling(\"div\").text.strip()\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            product_details[\"Body\"] = feature_chart.find(\"div\", string=\"Body\").find_next_sibling(\"div\").text.strip()\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            product_details[\"Aftertaste\"] = feature_chart.find(\"div\", string=\"Aftertaste\").find_next_sibling(\"div\").text.strip()\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "    # Append to detailed product data\n",
    "    detailed_product_data.append(product_details)\n",
    "    print(f\"Scraped: {product['Title']}\")\n",
    "\n",
    "# Step 3: Write all details to CSV\n",
    "with open(output_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    fieldnames = [\n",
    "        \"Title\", \"Product URL\", \"Price\", \"Blend\", \"Acidity Level\",\n",
    "        \"Roast Level\", \"Flavor\", \"Body\", \"Aftertaste\", \"Region\",\n",
    "        \"Varietal\", \"Process\"\n",
    "    ]\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(detailed_product_data)\n",
    "\n",
    "print(f\"Product details successfully written to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
